{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git_lit import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying THUDM/ChatGLM2-6B\n",
      "Num Tokens: 18353\n",
      "Querying CASIA-IVA-Lab/FastSAM\n",
      "Num Tokens: 24502\n",
      "Querying princeton-vl/infinigen\n",
      "Num Tokens: 1356370\n",
      "Querying embedchain/embedchain\n",
      "Num Tokens: 8778\n",
      "Querying Stability-AI/generative-models\n",
      "Skipping file data/DejaVuSans.ttf due to UnicodeDecodeError\n",
      "Skipping file scripts/util/detection/p_head_v1.npz due to UnicodeDecodeError\n",
      "Skipping file scripts/util/detection/w_head_v1.npz due to UnicodeDecodeError\n",
      "Num Tokens: 74804\n",
      "Querying alexbei/telegram-groups\n",
      "Num Tokens: 49678\n",
      "Querying s0md3v/sd-webui-roop\n",
      "Num Tokens: 11492\n",
      "Querying OpenLMLab/LOMO\n",
      "Num Tokens: 30066\n",
      "Querying techleadhd/chatgpt-retrieval\n",
      "Num Tokens: 555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('.gitignore\\n\\nconstants.py\\n__pycache__\\npersist/\\n.DS_Store\\n.chroma/\\n\\n\\nREADME.md\\n\\n# chatgpt-retrieval\\n\\nSimple script to use ChatGPT on your own files.\\n\\nHere\\'s the [YouTube Video](https://youtu.be/9AXP7tCI9PI).\\n\\n## Installation\\n\\nInstall [Langchain](https://github.com/hwchase17/langchain) and other required packages.\\n```\\npip install langchain openai chromadb tiktoken unstructured\\n```\\nModify `constants.py` to use your own [OpenAI API key](https://platform.openai.com/account/api-keys).\\n\\nPlace your own data into `data/data.txt`.\\n\\n## Example usage\\nTest reading `data/data.txt` file.\\n```\\n> python chatgpt.py \"what is my dog\\'s name\"\\nYour dog\\'s name is Sunny.\\n```\\n\\nTest reading `data/cat.pdf` file.\\n```\\n> python chatgpt.py \"what is my cat\\'s name\"\\nYour cat\\'s name is Muffy.\\n```\\n\\n\\nchatgpt.py\\n\\nimport os\\nimport sys\\n\\nimport openai\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.indexes import VectorstoreIndexCreator\\nfrom langchain.indexes.vectorstore import VectorStoreIndexWrapper\\nfrom langchain.llms import OpenAI\\nfrom langchain.vectorstores import Chroma\\n\\nimport constants\\n\\nos.environ[\"OPENAI_API_KEY\"] = constants.APIKEY\\n\\n# Enable to save to disk & reuse the model (for repeated queries on the same data)\\nPERSIST = False\\n\\nquery = sys.argv[1]\\n\\nif PERSIST and os.path.exists(\"persist\"):\\n  print(\"Reusing index...\\\\n\")\\n  vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\\n  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\\nelse:\\n  #loader = TextLoader(\"data/data.txt\") # Use this line if you only need data.txt\\n  loader = DirectoryLoader(\"data/\")\\n  if PERSIST:\\n    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\\n  else:\\n    index = VectorstoreIndexCreator().from_loaders([loader])\\n\\nchain = RetrievalQA.from_chain_type(\\n  llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\\n  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\\n)\\nprint(chain.run(query))\\n\\n\\nconstants.py\\n\\nAPIKEY = \"<your OpenAI API key>\"\\n',\n",
       " {'.gitignore': 'constants.py\\n__pycache__\\npersist/\\n.DS_Store\\n.chroma/\\n',\n",
       "  'README.md': '# chatgpt-retrieval\\n\\nSimple script to use ChatGPT on your own files.\\n\\nHere\\'s the [YouTube Video](https://youtu.be/9AXP7tCI9PI).\\n\\n## Installation\\n\\nInstall [Langchain](https://github.com/hwchase17/langchain) and other required packages.\\n```\\npip install langchain openai chromadb tiktoken unstructured\\n```\\nModify `constants.py` to use your own [OpenAI API key](https://platform.openai.com/account/api-keys).\\n\\nPlace your own data into `data/data.txt`.\\n\\n## Example usage\\nTest reading `data/data.txt` file.\\n```\\n> python chatgpt.py \"what is my dog\\'s name\"\\nYour dog\\'s name is Sunny.\\n```\\n\\nTest reading `data/cat.pdf` file.\\n```\\n> python chatgpt.py \"what is my cat\\'s name\"\\nYour cat\\'s name is Muffy.\\n```\\n',\n",
       "  'chatgpt.py': 'import os\\nimport sys\\n\\nimport openai\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.indexes import VectorstoreIndexCreator\\nfrom langchain.indexes.vectorstore import VectorStoreIndexWrapper\\nfrom langchain.llms import OpenAI\\nfrom langchain.vectorstores import Chroma\\n\\nimport constants\\n\\nos.environ[\"OPENAI_API_KEY\"] = constants.APIKEY\\n\\n# Enable to save to disk & reuse the model (for repeated queries on the same data)\\nPERSIST = False\\n\\nquery = sys.argv[1]\\n\\nif PERSIST and os.path.exists(\"persist\"):\\n  print(\"Reusing index...\\\\n\")\\n  vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\\n  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\\nelse:\\n  #loader = TextLoader(\"data/data.txt\") # Use this line if you only need data.txt\\n  loader = DirectoryLoader(\"data/\")\\n  if PERSIST:\\n    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\\n  else:\\n    index = VectorstoreIndexCreator().from_loaders([loader])\\n\\nchain = RetrievalQA.from_chain_type(\\n  llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\\n  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\\n)\\nprint(chain.run(query))\\n',\n",
       "  'constants.py': 'APIKEY = \"<your OpenAI API key>\"\\n'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate.search_for_repo(last_n_days=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
